import pandas as pd
import datetime
from pathlib import Path
from typing import List, Dict, Any, Iterator, Union, cast, Optional
from tqdm.auto import tqdm
import gzip
import pyarrow as pa
import pyarrow.parquet as pq
import gc

class SnapshotParser:
    """TWSE Snapshot è§£æå™¨ - æ”¯æ´ä¸²æµå¼è™•ç†(stream-to-disk)é¿å…OOM"""
    
    def __init__(self):
        self.categorical_fields = ['remark', 'trend_flag', 'match_flag', 
                                 'trade_upper_lower_limit', 'buy_upper_lower_limit', 
                                 'sell_upper_lower_limit']
    
    @staticmethod
    def parse_6digit_price(value: str) -> float:
        """å°‡6ä½æ•¸å­—ä¸²è½‰æ›ç‚ºåƒ¹æ ¼ï¼ˆä¿ç•™2ä½å°æ•¸ï¼Œä¾‹å¦‚ "002900" -> 29.0ï¼‰"""
        return int(value.strip()) / 100.0 if value.strip() else 0.0
    
    @staticmethod
    def parse_time_hhmmss(value: str) -> datetime.time:
        """è§£æ HHMMSS æ™‚é–“æ ¼å¼"""
        value = value.strip()[:6]  # å–å‰6ä½æ•¸å­—
        if len(value) < 6:
            value = value.ljust(6, '0')
        hh, mm, ss = int(value[0:2]), int(value[2:4]), int(value[4:6])
        return datetime.time(hour=hh, minute=mm, second=ss)
    
    @staticmethod
    def parse_date_yyyymmdd(value: str) -> datetime.date:
        """è§£æ YYYYMMDD æ—¥æœŸæ ¼å¼"""
        return datetime.datetime.strptime(value.strip(), "%Y%m%d").date()
    
    @staticmethod
    def parse_5_price_volume(value: str) -> List[Dict[str, float]]:
        """è§£æ70å­—ç¬¦æ¬„ä½ï¼ŒåŒ…å«5å° (åƒ¹æ ¼(6), æˆäº¤é‡(8))"""
        pairs = []
        if len(value) != 70:
            return [{'price': 0.0, 'volume': 0} for _ in range(5)]
        
        for i in range(5):
            start = i * 14
            price_str = value[start:start + 6]
            volume_str = value[start + 6:start + 14]
            
            price = SnapshotParser.parse_6digit_price(price_str)
            volume = int(volume_str) if volume_str.strip() else 0
            pairs.append({'price': price, 'volume': volume})
        
        return pairs
    
    def parse_snapshot_line(self, line: str) -> Dict[str, Any]:
        """è§£æå–®è¡Œå¿«ç…§æ•¸æ“šï¼ˆæ”¯æ´186å’Œ190å­—ç¯€æ ¼å¼ï¼‰"""
        line_len = len(line)
        
        if line_len == 190:
            # æ–°æ ¼å¼ (190 å­—ç¯€)
            record = {
                'securities_code': line[0:6].strip(),
                'display_time_raw': line[6:18],  # 12 å­—ç¯€
                'remark': line[18:19],
                'trend_flag': line[19:20],
                'match_flag': line[20:21],
                'trade_upper_lower_limit': line[21:22],
                'trade_price': self.parse_6digit_price(line[22:28]),
                'transaction_volume': int(line[28:36]) if line[28:36].strip() else 0,
                'buy_tick_size': int(line[36:37]) if line[36:37].strip() else 0,
                'buy_upper_lower_limit': line[37:38],
                'buy_5_price_volume': self.parse_5_price_volume(line[38:108]),
                'sell_tick_size': int(line[108:109]) if line[108:109].strip() else 0,
                'sell_upper_lower_limit': line[109:110],
                'sell_5_price_volume': self.parse_5_price_volume(line[110:180]),
                'display_date_raw': line[180:188],
                'match_staff': line[188:190]
            }
            # æ–°æ ¼å¼å–12å­—ç¯€æ™‚é–“æ¬„ä½çš„å‰6ä½
            time_str = record['display_time_raw'][:6]
        
        elif line_len == 186:
            # èˆŠæ ¼å¼ (186 å­—ç¯€)
            record = {
                'securities_code': line[0:6].strip(),
                'display_time_raw': line[6:14],  # 8 å­—ç¯€
                'remark': line[14:15],
                'trend_flag': line[15:16],
                'match_flag': line[16:17],
                'trade_upper_lower_limit': line[17:18],
                'trade_price': self.parse_6digit_price(line[18:24]),
                'transaction_volume': int(line[24:32]) if line[24:32].strip() else 0,
                'buy_tick_size': int(line[32:33]) if line[32:33].strip() else 0,
                'buy_upper_lower_limit': line[33:34],
                'buy_5_price_volume': self.parse_5_price_volume(line[34:104]),
                'sell_tick_size': int(line[104:105]) if line[104:105].strip() else 0,
                'sell_upper_lower_limit': line[105:106],
                'sell_5_price_volume': self.parse_5_price_volume(line[106:176]),
                'display_date_raw': line[176:184],
                'match_staff': line[184:186]
            }
            time_str = record['display_time_raw']
        
        else:
            raise ValueError(f"æœªé æœŸçš„è¡Œé•·åº¦: {line_len}. é æœŸç‚º 186 æˆ– 190 å­—ç¯€.")
        
        # è§£ææ—¥æœŸå’Œæ™‚é–“ï¼Œç„¶å¾Œåˆä½µ
        display_date = self.parse_date_yyyymmdd(record['display_date_raw'])
        display_time = self.parse_time_hhmmss(time_str)
        record['datetime'] = datetime.datetime.combine(display_date, display_time)
        
        # ç§»é™¤åŸå§‹æ—¥æœŸ/æ™‚é–“æ¬„ä½
        del record['display_date_raw']
        del record['display_time_raw']
        
        return record
    
    def flatten_snapshot(self, record: Dict[str, Any]) -> Dict[str, Any]:
        """å°‡5æª”åƒ¹é‡æ•¸æ“šå±•å¹³ç‚ºç¨ç«‹æ¬„ä½"""
        flattened = record.copy()
        
        # ç§»é™¤åŸå§‹åµŒå¥—æ•¸æ“š
        buy_data = flattened.pop('buy_5_price_volume', [])
        sell_data = flattened.pop('sell_5_price_volume', [])
        
        # å±•å¹³è²·ç›¤æ•¸æ“š (bid)
        for i, pair in enumerate(buy_data, 1):
            flattened[f'bid_price_{i}'] = pair['price']
            flattened[f'bid_volume_{i}'] = pair['volume']
        
        # å±•å¹³è³£ç›¤æ•¸æ“š (ask)  
        for i, pair in enumerate(sell_data, 1):
            flattened[f'ask_price_{i}'] = pair['price']
            flattened[f'ask_volume_{i}'] = pair['volume']
        
        return flattened
    
    def load_dsp_file(self, filepath: str, use_categorical: bool = True) -> pd.DataFrame:
        """è¼‰å…¥ä¸¦è§£æDSPæª”æ¡ˆï¼ˆç›¸å®¹æ€§ä¿ç•™ï¼Œå»ºè­°ä½¿ç”¨ä¸²æµæ–¹æ³•ï¼‰"""
        return cast(pd.DataFrame, self.load_dsp_file_lazy(
            filepath,
            use_categorical=use_categorical,
            chunk_size=None,
        ))

    def load_dsp_file_lazy(
        self,
        filepath: str,
        *,
        use_categorical: bool = True,
        chunk_size: int | None = None,
    ) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:
        """æ‡¶è¼‰å…¥DSPæª”æ¡ˆï¼ˆæ”¯æ´åˆ†å¡Šè™•ç†ï¼‰"""

        def _open_file(path: str):
            """è¼”åŠ©å‡½æ•¸ï¼šé–‹å•Ÿä¸€èˆ¬æˆ–gzipæª”æ¡ˆ"""
            return gzip.open(path, "rb") if path.endswith(".gz") else open(path, "rb")

        def _apply_categorical(df: pd.DataFrame):
            if use_categorical:
                for field in self.categorical_fields:
                    if field in df.columns:
                        df[field] = df[field].astype("category")
            return df

        if chunk_size is None:
            # éæ‡¶è¼‰å…¥ï¼šå›å‚³å–®ä¸€DataFrameï¼ˆä¿ç•™åŸæœ‰è¡Œç‚ºï¼‰
            records: list[dict] = []
            
            # è¨ˆç®—æª”æ¡ˆå¤§å°ç”¨æ–¼é€²åº¦æ¢
            file_size = Path(filepath).stat().st_size
            
            with _open_file(filepath) as f:
                with tqdm(total=file_size, unit='B', unit_scale=True, 
                         desc="è®€å–DSPæª”æ¡ˆ") as pbar:
                    for raw_line in f:
                        pbar.update(len(raw_line))
                        line = raw_line.rstrip(b"\r\n")
                        if len(line) not in {186, 190}:
                            continue
                        decoded_line = line.decode("utf-8", errors="replace")
                        record = self.parse_snapshot_line(decoded_line)
                        records.append(self.flatten_snapshot(record))

            df = _apply_categorical(pd.DataFrame(records))
            return df

        # æ‡¶è¼‰å…¥æ¨¡å¼ï¼šç”¢ç”ŸDataFrameåˆ†å¡Š
        def _chunk_generator() -> Iterator[pd.DataFrame]:
            chunk_records: list[dict] = []
            file_size = Path(filepath).stat().st_size
            
            with _open_file(filepath) as f:
                with tqdm(total=file_size, unit='B', unit_scale=True, 
                         desc="è®€å–DSPæª”æ¡ˆ (æ‡¶è¼‰å…¥)") as pbar:
                    for raw_line in f:
                        pbar.update(len(raw_line))
                        line = raw_line.rstrip(b"\r\n")
                        if len(line) not in {186, 190}:
                            continue
                        decoded_line = line.decode("utf-8", errors="replace")
                        record = self.parse_snapshot_line(decoded_line)
                        chunk_records.append(self.flatten_snapshot(record))

                        if len(chunk_records) >= chunk_size:
                            df_chunk = _apply_categorical(pd.DataFrame(chunk_records))
                            yield df_chunk
                            chunk_records.clear()

                    # ç”¢ç”Ÿå‰©é¤˜æ•¸æ“š
                    if chunk_records:
                        yield _apply_categorical(pd.DataFrame(chunk_records))

        return _chunk_generator()
    
    def stream_dsp_to_parquet(
        self,
        src_path: str,
        dst_path: str,
        *,
        chunk_size: int = 200_000,          # è¡Œæ•¸æ±ºå®š row-group å¤§å°
        compression: str = "zstd",
        compression_level: int = 3,
        show_progress: bool = True,
    ) -> None:
        """
        é‚Šè®€é‚Šå¯« DSP â†’ Parquetï¼Œé¿å…OOMå•é¡Œ
        
        Parameters
        ----------
        src_path : str
            ä¾†æºDSPæª”æ¡ˆè·¯å¾‘ï¼ˆæ”¯æ´.gzå£“ç¸®ï¼‰
        dst_path : str
            ç›®æ¨™Parquetæª”æ¡ˆè·¯å¾‘
        chunk_size : int, default 200_000
            æ¯å€‹row groupçš„è¡Œæ•¸ï¼Œå½±éŸ¿è¨˜æ†¶é«”ä½¿ç”¨é‡
        compression : str, default "zstd"
            å£“ç¸®æ ¼å¼ï¼ˆæ¨è–¦ zstd æˆ– lz4ï¼‰
        compression_level : int, default 3
            å£“ç¸®ç­‰ç´šï¼ˆ1-22ï¼‰
        show_progress : bool, default True
            æ˜¯å¦é¡¯ç¤ºé€²åº¦æ¢
        """
        dst_path_obj = Path(dst_path)
        dst_path_obj.parent.mkdir(parents=True, exist_ok=True)

        writer: Optional[pq.ParquetWriter] = None
        total_rows = 0

        try:
            # ä½¿ç”¨æ‡¶è¼‰å…¥ç”Ÿæˆå™¨è™•ç†æ•¸æ“š
            chunk_iter = self.load_dsp_file_lazy(
                src_path, 
                chunk_size=chunk_size, 
                use_categorical=False  # Arrowä¸éœ€è¦pandas categorical
            )
            
            progress_desc = f"ä¸²æµå¯«å…¥ {dst_path_obj.name}"
            if show_progress:
                # åŒ…è£è¿­ä»£å™¨ä»¥é¡¯ç¤ºè™•ç†äº†å¤šå°‘å€‹chunks
                chunk_iter = tqdm(chunk_iter, desc=progress_desc, unit="chunks")

            for chunk_df in chunk_iter:
                if len(chunk_df) == 0:
                    continue
                    
                # è½‰æ›ç‚ºArrow Table
                table = pa.Table.from_pandas(chunk_df, preserve_index=False)
                
                # åˆå§‹åŒ–writerï¼ˆç¬¬ä¸€å€‹chunkæ™‚ï¼‰
                if writer is None:
                    writer = pq.ParquetWriter(
                        dst_path_obj, 
                        table.schema,
                        compression=compression, 
                        compression_level=compression_level,
                        use_dictionary=True,  # å­—ä¸²å£“ç¸®å„ªåŒ–
                    )
                
                # å¯«å…¥ç•¶å‰chunk
                writer.write_table(table)
                total_rows += len(chunk_df)
                
                # ç«‹å³é‡‹æ”¾è¨˜æ†¶é«”
                del chunk_df, table
                gc.collect()  # å¼·åˆ¶åƒåœ¾å›æ”¶

        finally:
            if writer is not None:
                writer.close()
        
        print(f"âœ… å®Œæˆï¼å…±è™•ç† {total_rows:,} è¡Œæ•¸æ“š â†’ {dst_path_obj}")
        
        # é¡¯ç¤ºæª”æ¡ˆå¤§å°
        file_size_mb = dst_path_obj.stat().st_size / (1024 * 1024)
        print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆå¤§å°: {file_size_mb:.1f} MB")

    def stream_dsp_to_partitioned_parquet(
        self,
        src_path: str,
        base_output_dir: str,
        *,
        chunk_size: int = 200_000,
        compression: str = "zstd",
        compression_level: int = 3,
        show_progress: bool = True,
        max_open_files: int = 100,  # æ–°å¢ï¼šé™åˆ¶åŒæ™‚é–‹å•Ÿçš„æ–‡ä»¶æ•¸é‡
    ) -> None:
        """
        ä¸²æµå¼è™•ç†ä¸¦æŒ‰æ—¥æœŸ/è­‰åˆ¸ä»£ç¢¼åˆ†å‰²å­˜å„²
        
        é¿å…è¨˜æ†¶é«”çˆ†ç‚¸çš„åŒæ™‚ï¼Œç¶­æŒåŸæœ‰çš„åˆ†å‰²è¼¸å‡ºçµæ§‹
        ä½¿ç”¨æ‰¹è™•ç†ç­–ç•¥é¿å…åŒæ™‚é–‹å•Ÿå¤ªå¤šæ–‡ä»¶å°è‡´"Too many open files"éŒ¯èª¤
        
        Parameters
        ----------
        max_open_files : int, default 100
            åŒæ™‚é–‹å•Ÿçš„æœ€å¤§æ–‡ä»¶æ•¸é‡ï¼Œé¿å…è¶…éç³»çµ±é™åˆ¶
        """
        base_path = Path(base_output_dir)
        
        # ä½¿ç”¨ç·©è¡å€ç­–ç•¥ï¼Œæ”¶é›†æ•¸æ“šå¾Œæ‰¹é‡å¯«å…¥
        data_buffer: Dict[tuple, List[pd.DataFrame]] = {}  # (date, securities_code) -> [dataframes]
        total_rows = 0
        processed_files = set()  # è¿½è¹¤å·²è™•ç†çš„æ–‡ä»¶

        def _flush_buffer_batch(buffer_subset: Dict[tuple, List[pd.DataFrame]]) -> None:
            """æ‰¹é‡å¯«å…¥ç·©è¡å€ä¸­çš„æ•¸æ“š"""
            writers: Dict[tuple, pq.ParquetWriter] = {}
            
            try:
                for key, df_list in buffer_subset.items():
                    if not df_list:
                        continue
                        
                    date, securities_code = key
                    
                    # å»ºç«‹è¼¸å‡ºè·¯å¾‘
                    date_dir = base_path / str(date)
                    date_dir.mkdir(parents=True, exist_ok=True)
                    file_path = date_dir / f"{securities_code}.parquet"
                    
                    # åˆä½µåŒä¸€è‚¡ç¥¨çš„æ‰€æœ‰DataFrame
                    combined_df = pd.concat(df_list, ignore_index=True)
                    table = pa.Table.from_pandas(combined_df, preserve_index=False)
                    
                    # ç¢ºå®šæ–‡ä»¶å¯«å…¥æ¨¡å¼
                    if key in processed_files:
                        # æª”æ¡ˆå·²å­˜åœ¨ï¼Œéœ€è¦è®€å–ç¾æœ‰æ•¸æ“šä¸¦åˆä½µ
                        if file_path.exists():
                            existing_table = pq.read_table(file_path)
                            combined_table = pa.concat_tables([existing_table, table])
                        else:
                            combined_table = table
                    else:
                        combined_table = table
                        processed_files.add(key)
                    
                    # é‡æ–°å¯«å…¥å®Œæ•´æ•¸æ“š
                    pq.write_table(
                        combined_table,
                        file_path,
                        compression=compression,
                        compression_level=compression_level,
                        use_dictionary=True,
                    )
                    
                    del combined_df, table, combined_table
                    
            finally:
                # æ¸…ç†writersï¼ˆé›–ç„¶åœ¨é€™å€‹å¯¦ç¾ä¸­ä¸éœ€è¦ï¼‰
                for writer in writers.values():
                    if hasattr(writer, 'close'):
                        writer.close()

        try:
            chunk_iter = self.load_dsp_file_lazy(
                src_path, 
                chunk_size=chunk_size, 
                use_categorical=False
            )
            
            if show_progress:
                chunk_iter = tqdm(chunk_iter, desc="ä¸²æµå¼åˆ†å‰²å¯«å…¥", unit="chunks")

            for chunk_df in chunk_iter:
                if len(chunk_df) == 0:
                    continue
                
                # æ·»åŠ æ—¥æœŸæ¬„ä½ç”¨æ–¼åˆ†çµ„
                chunk_df = chunk_df.copy()
                chunk_df['date'] = chunk_df['datetime'].dt.date
                
                # æŒ‰ (æ—¥æœŸ, è­‰åˆ¸ä»£ç¢¼) åˆ†çµ„è™•ç†
                grouped = chunk_df.groupby(['date', 'securities_code'])
                
                for name, group_df in grouped:
                    date, securities_code = cast(tuple, name)
                    key = (date, securities_code)
                    
                    # ç§»é™¤è‡¨æ™‚æ—¥æœŸæ¬„ä½
                    group_df_clean = group_df.drop('date', axis=1)
                    
                    # åŠ å…¥ç·©è¡å€
                    if key not in data_buffer:
                        data_buffer[key] = []
                    data_buffer[key].append(group_df_clean.copy())
                    
                    total_rows += len(group_df_clean)
                    
                    del group_df_clean

                # æª¢æŸ¥æ˜¯å¦éœ€è¦æ‰¹é‡å¯«å…¥
                if len(data_buffer) >= max_open_files:
                    # å–å‰ max_open_files å€‹é€²è¡Œå¯«å…¥
                    keys_to_process = list(data_buffer.keys())[:max_open_files]
                    buffer_subset = {k: data_buffer[k] for k in keys_to_process}
                    
                    _flush_buffer_batch(buffer_subset)
                    
                    # æ¸…ç†å·²è™•ç†çš„ç·©è¡å€
                    for k in keys_to_process:
                        del data_buffer[k]
                    
                    gc.collect()

                del chunk_df

            # è™•ç†å‰©é¤˜çš„ç·©è¡å€æ•¸æ“š
            if data_buffer:
                _flush_buffer_batch(data_buffer)

        finally:
            # ç¢ºä¿æ¸…ç†æ‰€æœ‰è³‡æº
            data_buffer.clear()
            gc.collect()
        
        print(f"âœ… å®Œæˆåˆ†å‰²å­˜å„²ï¼å…±è™•ç† {total_rows:,} è¡Œæ•¸æ“š")
        print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {base_path}")
        print(f"ğŸ“Š ç”Ÿæˆæª”æ¡ˆæ•¸: {len(processed_files)} å€‹")

    def save_by_securities(self, df: pd.DataFrame, output_dir: str) -> None:
        """æŒ‰è­‰åˆ¸ä»£ç¢¼åˆ†çµ„ä¿å­˜ï¼ˆç›¸å®¹æ€§ä¿ç•™ï¼Œå»ºè­°ä½¿ç”¨ä¸²æµæ–¹æ³•ï¼‰"""
        output_path = Path(output_dir)
        
        # æ·»åŠ æ—¥æœŸæ¬„ä½ç”¨æ–¼åˆ†çµ„
        df_with_date = df.copy()
        df_with_date['date'] = df_with_date['datetime'].dt.date
        
        grouped = df_with_date.groupby(['date', 'securities_code'])

        print("ä¿å­˜åˆ†çµ„parquetæª”æ¡ˆ...")

        for name, group_df in grouped:
            date_str, securities_code = cast(tuple, name)

            # å»ºç«‹ç›®éŒ„çµæ§‹: output_dir/YYYY-MM-DD/
            date_dir = output_path / str(date_str)
            date_dir.mkdir(parents=True, exist_ok=True)
            
            # ç§»é™¤è‡¨æ™‚æ—¥æœŸæ¬„ä½
            group_df_clean = group_df.drop('date', axis=1)
            
            # ä¿å­˜ç‚º securities_code.parquet
            file_path = date_dir / f"{securities_code}.parquet"
            group_df_clean.to_parquet(file_path, index=False)
            
        print(f"æ•¸æ“šå·²ä¿å­˜è‡³ {output_path}")


def main():
    """ä½¿ç”¨ç¯„ä¾‹ - å±•ç¤ºä¸²æµå¼è™•ç†èƒ½åŠ›"""
    parser = SnapshotParser()
    
    print("ğŸš€ TWSE Snapshot ä¸²æµå¼è™•ç†å™¨ v3")
    print("=" * 50)
    
    # æ–¹æ³•1: ä¸²æµå¯«å…¥å–®ä¸€Parquetæª”æ¡ˆï¼ˆæ¨è–¦ç”¨æ–¼å¾ŒçºŒåˆ†æï¼‰
    print("ğŸ“¥ æ–¹æ³•1: ä¸²æµå¯«å…¥å–®ä¸€æª”æ¡ˆ...")
    parser.stream_dsp_to_parquet(
        "snapshot/Sample_new.gz",
        "output/new_format_streamed.parquet",
        chunk_size=250_000,  # èª¿æ•´ä»¥å¹³è¡¡è¨˜æ†¶é«”ä½¿ç”¨å’ŒI/Oæ•ˆç‡
    )
    
    print("\n" + "-" * 30 + "\n")
    
    # æ–¹æ³•2: ä¸²æµå¼åˆ†å‰²å­˜å„²ï¼ˆç›¸å®¹åŸæœ‰çµæ§‹ï¼‰
    print("ğŸ“‚ æ–¹æ³•2: ä¸²æµå¼åˆ†å‰²å­˜å„²...")
    parser.stream_dsp_to_partitioned_parquet(
        "snapshot/Sample_new.gz",
        "output/new_format_partitioned",
        chunk_size=200_000,
    )
    
    print("\nğŸ‰ è™•ç†å®Œæˆï¼è¨˜æ†¶é«”ä½¿ç”¨é‡å·²å¤§å¹…é™ä½")
    print("ğŸ’¡ æç¤ºï¼šchunk_size å¯ä¾æ“šå¯ç”¨è¨˜æ†¶é«”èª¿æ•´")
    print("   - è¼ƒå¤§å€¼ï¼šæ›´å¥½çš„I/Oæ•ˆç‡ï¼Œä½†éœ€æ›´å¤šè¨˜æ†¶é«”")
    print("   - è¼ƒå°å€¼ï¼šæ›´ä½è¨˜æ†¶é«”ä½¿ç”¨ï¼Œä½†I/Oæ¬¡æ•¸å¢åŠ ")


if __name__ == "__main__":
    main()
