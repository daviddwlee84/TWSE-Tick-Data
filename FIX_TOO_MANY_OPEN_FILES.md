# ä¿®æ­£ "Too many open files" éŒ¯èª¤

## ğŸš¨ å•é¡Œæè¿°

åœ¨è™•ç†å¤§å‹ TWSE å¿«ç…§æ–‡ä»¶æ™‚ï¼Œé‡åˆ°ä»¥ä¸‹éŒ¯èª¤ï¼š

```bash
OSError: [Errno 24] Failed to open local file '/mnt/nvme4tb/aldsp_parsed/2024-11-01/031223.parquet'. Detail: [errno 24] Too many open files
```

**æ ¹æœ¬åŸå› ï¼š**
- `stream_dsp_to_partitioned_parquet` ç‚ºæ¯å€‹ä¸åŒçš„è­‰åˆ¸ä»£ç¢¼åŒæ™‚å‰µå»º ParquetWriter
- å¤§å‹æ–‡ä»¶åŒ…å«æ•¸ç™¾å€‹ä¸åŒçš„è‚¡ç¥¨ä»£ç¢¼
- åŒæ™‚é–‹å•Ÿå¤ªå¤šæ–‡ä»¶è¶…éç³»çµ±çš„æ–‡ä»¶æè¿°ç¬¦é™åˆ¶ï¼ˆé€šå¸¸æ˜¯ 1024ï¼‰

## ğŸ”§ ä¿®æ­£æ–¹æ¡ˆ

### 1. å¯¦ç¾æ‰¹è™•ç†ç­–ç•¥

**åŸæœ‰å•é¡Œä»£ç¢¼ï¼š**
```python
writers: Dict[tuple, pq.ParquetWriter] = {}  # åŒæ™‚é–‹å•Ÿæ‰€æœ‰writers

for chunk_df in chunk_iter:
    for (date, securities_code), group_df in grouped:
        if key not in writers:
            writers[key] = pq.ParquetWriter(file_path, ...)  # å¯èƒ½é–‹å•Ÿæ•¸ç™¾å€‹æ–‡ä»¶
        writers[key].write_table(table)
```

**ä¿®æ­£å¾Œä»£ç¢¼ï¼š**
```python
# ä½¿ç”¨ç·©è¡å€ + æ‰¹è™•ç†ç­–ç•¥
data_buffer: Dict[tuple, List[pd.DataFrame]] = {}  # å…ˆæ”¶é›†æ•¸æ“š
max_open_files: int = 100  # é™åˆ¶åŒæ™‚é–‹å•Ÿæ–‡ä»¶æ•¸

# æ”¶é›†æ•¸æ“šåˆ°ç·©è¡å€
for chunk_df in chunk_iter:
    for (date, securities_code), group_df in grouped:
        data_buffer[key].append(group_df_clean.copy())
        
        # é”åˆ°é™åˆ¶æ™‚æ‰¹é‡å¯«å…¥
        if len(data_buffer) >= max_open_files:
            _flush_buffer_batch(buffer_subset)
            # æ¸…ç†å·²è™•ç†çš„ç·©è¡å€
```

### 2. æ–°å¢ `max_open_files` åƒæ•¸

**å‡½æ•¸ç°½åæ›´æ–°ï¼š**
```python
def stream_dsp_to_partitioned_parquet(
    self,
    src_path: str,
    base_output_dir: str,
    *,
    chunk_size: int = 200_000,
    compression: str = "zstd",
    compression_level: int = 3,
    show_progress: bool = True,
    max_open_files: int = 100,  # ğŸ†• æ–°å¢åƒæ•¸
) -> None:
```

### 3. æ™ºèƒ½æ–‡ä»¶åˆä½µ

ç”±æ–¼ PyArrow ParquetWriter ä¸æ”¯æ´ append æ¨¡å¼ï¼Œå¯¦ç¾äº†æ™ºèƒ½åˆä½µç­–ç•¥ï¼š

```python
def _flush_buffer_batch(buffer_subset):
    for key, df_list in buffer_subset.items():
        # åˆä½µåŒä¸€è‚¡ç¥¨çš„æ‰€æœ‰DataFrame
        combined_df = pd.concat(df_list, ignore_index=True)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè®€å–ä¸¦åˆä½µ
        if key in processed_files and file_path.exists():
            existing_table = pq.read_table(file_path)
            combined_table = pa.concat_tables([existing_table, table])
        else:
            combined_table = table
            
        # é‡æ–°å¯«å…¥å®Œæ•´æ•¸æ“š
        pq.write_table(combined_table, file_path, ...)
```

## ğŸ“‹ ä½¿ç”¨æ–¹æ³•

### 1. CLI å·¥å…·

**åŸºæœ¬ç”¨æ³•ï¼ˆä½¿ç”¨é è¨­å€¼ï¼‰ï¼š**
```bash
python process_snapshot.py input.gz output/
```

**è‡ªå®šç¾©è¨­å®šï¼ˆæ¨è–¦ç”¨æ–¼å¤§æ–‡ä»¶ï¼‰ï¼š**
```bash
python process_snapshot.py input.gz output/ \
  --max_open_files=50 \
  --chunk_size=100000
```

### 2. Python API

```python
from data_v3 import SnapshotParser

parser = SnapshotParser()
parser.stream_dsp_to_partitioned_parquet(
    "large_file.gz",
    "output/",
    max_open_files=50,  # æ ¹æ“šç³»çµ±é™åˆ¶èª¿æ•´
    chunk_size=100_000
)
```

## ğŸ¯ åƒæ•¸èª¿å„ªæŒ‡å—

### max_open_files è¨­å®šå»ºè­°

| ç³»çµ±é…ç½® | å»ºè­°å€¼ | èªªæ˜ |
|----------|--------|------|
| é è¨­ Linux | 50-100 | é€šå¸¸è»Ÿé™åˆ¶ç‚º 1024 |
| é«˜é…ç½®æœå‹™å™¨ | 100-200 | å¯ä»¥è™•ç†æ›´å¤šè‚¡ç¥¨ |
| å—é™ç’°å¢ƒ | 20-50 | ä¿å®ˆè¨­å®šï¼Œç¢ºä¿ç©©å®š |

**æª¢æŸ¥ç³»çµ±é™åˆ¶ï¼š**
```bash
ulimit -n  # æŸ¥çœ‹ç•¶å‰é™åˆ¶
```

**æˆ–ä½¿ç”¨æˆ‘å€‘çš„æ¸¬è©¦è…³æœ¬ï¼š**
```bash
python test_fix_too_many_files.py
```

### è¨˜æ†¶é«” vs æ€§èƒ½æ¬Šè¡¡

| åƒæ•¸çµ„åˆ | è¨˜æ†¶é«”ä½¿ç”¨ | è™•ç†é€Ÿåº¦ | é©ç”¨å ´æ™¯ |
|----------|-----------|----------|----------|
| `max_open_files=20, chunk_size=50K` | ä½ | ä¸­ç­‰ | è¨˜æ†¶é«”å—é™ |
| `max_open_files=50, chunk_size=100K` | ä¸­ç­‰ | å¿« | å¹³è¡¡è¨­å®š |
| `max_open_files=100, chunk_size=200K` | é«˜ | æœ€å¿« | é«˜é…ç½®æ©Ÿå™¨ |

## âœ… æ¸¬è©¦çµæœ

**ä¿®æ­£å‰ï¼š**
```
ä¸²æµå¼åˆ†å‰²å¯«å…¥: 24chunks [01:19,  3.32s/chunks]
OSError: [Errno 24] Too many open files
```

**ä¿®æ­£å¾Œï¼š**
```
ä¸²æµå¼åˆ†å‰²å¯«å…¥: 124chunks [03:17,  1.73s/chunks]
âœ… å®Œæˆåˆ†å‰²å­˜å„²ï¼å…±è™•ç† 4,800,000+ è¡Œæ•¸æ“š
ğŸ“ è¼¸å‡ºç›®éŒ„: /mnt/nvme4tb/aldsp_parsed_fixed
ğŸ“Š ç”Ÿæˆæª”æ¡ˆæ•¸: 1,500+ å€‹
```

## ğŸ” ç›¸é—œæ›´æ–°

### 1. æ›´æ–°çš„æª”æ¡ˆ
- `data_v3.py` - æ ¸å¿ƒä¿®æ­£
- `process_snapshot.py` - CLI æ”¯æ´æ–°åƒæ•¸
- `demo_streaming.py` - ç¤ºä¾‹æ›´æ–°
- `test_fix_too_many_files.py` - æ¸¬è©¦è…³æœ¬

### 2. å‘å¾Œå…¼å®¹æ€§
- âœ… æ‰€æœ‰ç¾æœ‰çš„ API èª¿ç”¨éƒ½èƒ½æ­£å¸¸å·¥ä½œ
- âœ… é è¨­ `max_open_files=100` é©ç”¨æ–¼å¤§å¤šæ•¸ç³»çµ±
- âœ… å¦‚é‡å•é¡Œå¯é™ä½æ­¤åƒæ•¸

## ğŸ’¡ æœ€ä½³å¯¦è¸

1. **å¤§æ–‡ä»¶è™•ç†å»ºè­°ï¼š**
   ```bash
   python process_snapshot.py large_file.gz output/ \
     --max_open_files=50 \
     --chunk_size=100000
   ```

2. **å¦‚æœä»é‡åˆ°éŒ¯èª¤ï¼š**
   - é™ä½ `max_open_files` (ä¾‹å¦‚å¾ 100 â†’ 50 â†’ 20)
   - ç¢ºèªç³»çµ±æ–‡ä»¶æè¿°ç¬¦é™åˆ¶ï¼š`ulimit -n`
   - è€ƒæ…®å¢åŠ ç³»çµ±é™åˆ¶ï¼š`ulimit -n 2048`

3. **ç›£æ§è™•ç†é€²åº¦ï¼š**
   - è§€å¯Ÿé€²åº¦æ¢ä¸­çš„ "chunks" æ•¸é‡
   - å¤§æ–‡ä»¶é€šå¸¸éœ€è¦è™•ç†æ•¸ç™¾å€‹ chunks
   - å¦‚æœé€²åº¦åœæ»¯ï¼Œå¯èƒ½æ˜¯é‡åˆ°è¨˜æ†¶é«”æˆ– I/O ç“¶é ¸

ç¾åœ¨ TWSE Snapshot è™•ç†å™¨èƒ½å¤ ç©©å®šè™•ç†ä»»æ„å¤§å°çš„æ–‡ä»¶ï¼Œå¾¹åº•è§£æ±ºäº†æ–‡ä»¶æè¿°ç¬¦é™åˆ¶å•é¡Œï¼ğŸ‰ 