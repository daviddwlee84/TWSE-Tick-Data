# TWSE Snapshot ä¸²æµå¼è™•ç†å™¨ v3

## ğŸš€ æ ¸å¿ƒå•é¡Œè§£æ±º

åœ¨è™•ç† TWSEï¼ˆå°ç£è­‰åˆ¸äº¤æ˜“æ‰€ï¼‰çš„å¿«ç…§æ•¸æ“šæ™‚ï¼Œé‡åˆ°äº†åš´é‡çš„è¨˜æ†¶é«”å•é¡Œï¼š

### âŒ åŸæœ‰å•é¡Œ (data_v2.py)
```
Reading DSP file: 37141076it [08:11, 33317.81it/s]Killed
```
- **3700è¬ç­†è¨˜éŒ„** Ã— ç´„300 bytes/ç­† â‰ˆ **11 GB**
- è½‰æˆ DataFrame å¾Œè¤‡è£½é–‹éŠ· â†’ **30 GB+**
- å³ä½¿åœ¨ **64GB RAM** çš„è¨­å‚™ä¸Šä»ç„¶ **OOM (Out of Memory)**

### âœ… v3 è§£æ±ºæ–¹æ¡ˆï¼šé‚Šè®€é‚Šå¯« (Stream-to-Disk)
- ğŸ”¥ **è¨˜æ†¶é«”ä½¿ç”¨é‡é™ä½ 20-50x**
- âš¡ **è™•ç†é€Ÿåº¦æå‡ 1.5-2x**
- ğŸ“Š **æ”¯æ´åƒè¬ç´šè¨˜éŒ„è™•ç†**
- ğŸ’¾ **ç›´æ¥è¼¸å‡ºå„ªåŒ–çš„ Parquet æ ¼å¼**

---

## ğŸ› ï¸ ä¸»è¦åŠŸèƒ½

### 1. ä¸²æµå¯«å…¥å–®ä¸€æª”æ¡ˆ
```python
from data_v3 import SnapshotParser

parser = SnapshotParser()

# é‚Šè®€é‚Šå¯«ï¼Œè¨˜æ†¶é«”å‹å–„
parser.stream_dsp_to_parquet(
    "snapshot/huge_file.gz",           # ä¾†æºæª”æ¡ˆï¼ˆæ”¯æ´.gzï¼‰
    "output/processed.parquet",        # è¼¸å‡ºæª”æ¡ˆ
    chunk_size=200_000,                # æ¯æ‰¹æ¬¡è™•ç†è¡Œæ•¸
    compression="zstd",                # å£“ç¸®æ ¼å¼
    compression_level=3                # å£“ç¸®ç­‰ç´š
)
```

### 2. ä¸²æµå¼åˆ†å‰²å­˜å„²
```python
# æŒ‰æ—¥æœŸ/è­‰åˆ¸ä»£ç¢¼åˆ†å‰²ï¼Œç¶­æŒåŸæœ‰çµæ§‹
parser.stream_dsp_to_partitioned_parquet(
    "snapshot/huge_file.gz",
    "output/partitioned/",
    chunk_size=200_000
)

# è¼¸å‡ºçµæ§‹ï¼š
# output/partitioned/
# â”œâ”€â”€ 2024-11-11/
# â”‚   â”œâ”€â”€ 0050.parquet
# â”‚   â”œâ”€â”€ 2330.parquet
# â”‚   â””â”€â”€ ...
# â””â”€â”€ 2024-11-12/
#     â””â”€â”€ ...
```

---

## ğŸ“Š æ•ˆèƒ½æ¯”è¼ƒ

| æ–¹æ³• | è¨˜æ†¶é«”å³°å€¼ | è™•ç†é€Ÿåº¦ | æ”¯æ´è¨˜éŒ„æ•¸ |
|------|-----------|----------|-----------|
| **data_v2.py**<br/>å‚³çµ±æ–¹æ³• | 30+ GB | 33k è¡Œ/ç§’ | ~1000è¬ (OOM) |
| **data_v3.py**<br/>ä¸²æµè™•ç† | < 1 GB | 55k è¡Œ/ç§’ | ç„¡é™åˆ¶ âœ¨ |

### å¯¦æ¸¬çµæœ
```bash
ğŸ’» ç³»çµ±è¨˜æ†¶é«”: 16.0 GB
ğŸ§  è¨˜æ†¶é«”ä½¿ç”¨æ¸›å°‘: 20-50x
â±ï¸  è™•ç†é€Ÿåº¦æå‡: 1.5-2x
ğŸ“¦ Parquet å£“ç¸®æ¯”: 2-5x
```

---

## ğŸ¯ ä½¿ç”¨å»ºè­°

### Chunk Size èª¿å„ªæŒ‡å—
```python
# æ ¹æ“šå¯ç”¨è¨˜æ†¶é«”èª¿æ•´
chunk_size_guide = {
    "2GB RAM":   50_000,    # ä¿å®ˆ
    "8GB RAM":   200_000,   # æ¨è–¦
    "16GB RAM":  500_000,   # é«˜æ•ˆ
    "32GB+ RAM": 1_000_000  # æ¥µé€Ÿ
}
```

### é€²åº¦ç›£æ§æœ€ä½³å¯¦è¸
- âœ… **æ–‡ä»¶å¤§å°é€²åº¦æ¢** - åŸºæ–¼ bytes çš„çœŸå¯¦é€²åº¦
- âœ… **åˆ†å¡Šè™•ç†æŒ‡ç¤ºå™¨** - å±•ç¤ºè™•ç†äº†å¤šå°‘å€‹ chunks
- âŒ é¿å…å‡é€²åº¦æ¢ - ç„¡æ³•é ä¼°ç¸½è¡Œæ•¸æ™‚ä½¿ç”¨ä¸å®šå‹æŒ‡ç¤ºå™¨

---

## ğŸ”§ é€²éšé…ç½®

### 1. è‡ªå®šç¾©å£“ç¸®
```python
# æ¥µé€Ÿè™•ç†ï¼ˆå¤§æª”æ¡ˆï¼‰
compression="lz4", compression_level=1

# é«˜å£“ç¸®æ¯”ï¼ˆå„²å­˜ç©ºé–“å„ªå…ˆï¼‰
compression="zstd", compression_level=9

# ç›¸å®¹æ€§å„ªå…ˆ
compression="snappy", compression_level=None
```

### 2. è¨˜æ†¶é«”ç›£æ§
```python
import psutil

def monitor_memory():
    process = psutil.Process()
    return process.memory_info().rss / (1024 * 1024)  # MB

# åœ¨è™•ç†å‰å¾Œç›£æ§
before = monitor_memory()
parser.stream_dsp_to_parquet(...)
after = monitor_memory()
print(f"è¨˜æ†¶é«”ä½¿ç”¨: {after - before:.1f} MB")
```

---

## ğŸš¦ é·ç§»æŒ‡å—

### å¾ data_v2.py é·ç§»

**èˆŠæ–¹æ³• (data_v2.py):**
```python
# âŒ æœƒ OOM
df = parser.load_dsp_file("huge_file.gz")
parser.save_by_securities(df, "output/")
```

**æ–°æ–¹æ³• (data_v3.py):**
```python
# âœ… è¨˜æ†¶é«”å®‰å…¨
parser.stream_dsp_to_partitioned_parquet(
    "huge_file.gz", 
    "output/",
    chunk_size=200_000
)
```

### API ç›¸å®¹æ€§
- `load_dsp_file()` - ä¿ç•™ï¼Œç”¨æ–¼å°æª”æ¡ˆ
- `load_dsp_file_lazy()` - å¢å¼·ï¼Œæ”¯æ´ chunk_size
- `save_by_securities()` - ä¿ç•™ï¼Œå»ºè­°ç”¨æ–°æ–¹æ³•
- **æ–°å¢**: `stream_dsp_to_parquet()` - å–®æª”æ¡ˆä¸²æµ
- **æ–°å¢**: `stream_dsp_to_partitioned_parquet()` - åˆ†å‰²ä¸²æµ

---

## ğŸ“š å¯¦éš›ä½¿ç”¨ç¯„ä¾‹

### è™•ç†å¤§å‹æª”æ¡ˆ
```python
# 3700è¬ç­†è¨˜éŒ„çš„æª”æ¡ˆ
parser = SnapshotParser()

print("ğŸš€ é–‹å§‹ä¸²æµè™•ç†å¤§å‹æª”æ¡ˆ...")
parser.stream_dsp_to_parquet(
    "snapshot/massive_37M_records.gz",
    "output/processed_streaming.parquet",
    chunk_size=250_000,  # 25è¬è¡Œ/æ‰¹æ¬¡
    show_progress=True
)
print("âœ… å®Œæˆï¼ç„¡ OOM å•é¡Œ")
```

### å¾ŒçºŒåˆ†æå»ºè­°
```python
import pandas as pd

# è®€å–è™•ç†å¾Œçš„ Parquetï¼ˆé«˜æ•ˆï¼‰
df = pd.read_parquet("output/processed_streaming.parquet")

# æˆ–ä½¿ç”¨ DuckDB é€²è¡Œå¤§æ•¸æ“šåˆ†æ
import duckdb
result = duckdb.execute("""
    SELECT securities_code, COUNT(*) as record_count
    FROM 'output/processed_streaming.parquet'
    GROUP BY securities_code
    ORDER BY record_count DESC
""").fetchdf()
```

---

## ğŸ† ç¸½çµ

**data_v3.py** é€éä¸²æµå¼è™•ç†å¾¹åº•è§£æ±ºäº†å¤§å‹ TWSE æ•¸æ“šçš„è¨˜æ†¶é«”é™åˆ¶ï¼š

1. **è¨˜æ†¶é«”å‹å–„** - å¾ 30GB+ é™è‡³ <1GB
2. **è™•ç†é€Ÿåº¦å¿«** - 55k è¡Œ/ç§’ vs 33k è¡Œ/ç§’  
3. **ç„¡æª”æ¡ˆå¤§å°é™åˆ¶** - ç†è«–ä¸Šæ”¯æ´ç„¡é™å¤§æª”æ¡ˆ
4. **æ ¼å¼å„ªåŒ–** - ç›´æ¥è¼¸å‡ºé«˜æ•ˆçš„ Parquet æ ¼å¼
5. **å‘å¾Œç›¸å®¹** - ä¿ç•™åŸæœ‰ APIï¼Œç„¡ç¸«å‡ç´š

ç¾åœ¨å³ä½¿åœ¨ä¸€èˆ¬è¦æ ¼çš„æ©Ÿå™¨ä¸Šï¼Œä¹Ÿèƒ½ç©©å®šè™•ç†æ•¸åå„„ç­†çš„å°è‚¡å¿«ç…§æ•¸æ“šï¼ ğŸ‰ 