# TWSE Binary Data Compression Analysis

## Problem Statement

TWSE snapshot files are very large:
- **Uncompressed**: ~8.2 GB per day
- **Compressed (tar.gz)**: ~397 MB per day
- **Compression ratio**: ~95.8% (20x smaller)

**Question**: Can we read compressed files directly without decompressing the entire file first?

## TL;DR - Recommendations

| Use Case                   | Recommended Format | Reason                                     |
| -------------------------- | ------------------ | ------------------------------------------ |
| **Production Backtesting** | Parquet/Feather    | Pre-processed, fastest access              |
| **Storage Archive**        | gzip (.gz)         | Best compression, streaming support        |
| **Fast Random Access**     | LZ4 (.lz4)         | Fastest decompression, sequential friendly |
| **Development/Debug**      | Uncompressed       | No overhead                                |

## Compression Formats Comparison

### 1. gzip (.gz) â­ **Current Best Choice**

```python
import gzip

# Sequential reading (streaming)
with gzip.open('dsp20241104.gz', 'rb') as f:
    while True:
        record = f.read(191)  # Read one record at a time
        if not record:
            break
        # Process record
```

**Pros:**
- âœ… **Native Python support** (built-in `gzip` module)
- âœ… **Excellent compression ratio** (~95%, 20x smaller)
- âœ… **Streaming support** - no need to decompress entire file
- âœ… **Sequential reading** - RAM usage = buffer size (~8KB default)
- âœ… **Standard format** - widely supported

**Cons:**
- âŒ **Slower decompression** (~50-100 MB/s)
- âŒ **No random access** - must read sequentially from start

**Performance Metrics:**
```
Compression:   ~100-150 MB/s (level 6)
Decompression: ~50-100 MB/s
Memory usage:  ~8-32 KB (buffer)
Compression ratio: 95.8% (397 MB from 8.2 GB)
```

**Best for:**
- Long-term storage
- Sequential data processing
- Network transfer
- Backup archives

---

### 2. LZ4 (.lz4) - Fastest Decompression

```python
import lz4.frame

# Sequential reading with LZ4
with lz4.frame.open('dsp20241104.lz4', 'rb') as f:
    while True:
        record = f.read(191)
        if not record:
            break
        # Process record
```

**Pros:**
- âœ… **Very fast decompression** (~500-800 MB/s, 5-10x faster than gzip)
- âœ… **Streaming support** - sequential reading
- âœ… **Low CPU overhead**
- âœ… **Low memory usage** (~1-4 MB)

**Cons:**
- âŒ **Lower compression ratio** (~70-80%, worse than gzip)
- âŒ **Requires external library** (`pip install lz4`)
- âŒ **Larger files** - ~1.6-2 GB vs 397 MB for gzip

**Performance Metrics:**
```
Compression:   ~300-500 MB/s
Decompression: ~500-800 MB/s (10x faster than gzip!)
Memory usage:  ~1-4 MB
Compression ratio: ~75% (2 GB from 8.2 GB)
```

**Best for:**
- Real-time data processing
- Frequent re-reading
- CPU-constrained environments
- When speed > compression ratio

---

### 3. Zstandard (.zst) - Balanced

```python
import zstandard as zstd

# Sequential reading with Zstd
dctx = zstd.ZstdDecompressor()
with open('dsp20241104.zst', 'rb') as compressed:
    with dctx.stream_reader(compressed) as reader:
        while True:
            record = reader.read(191)
            if not record:
                break
            # Process record
```

**Pros:**
- âœ… **Excellent compression** (~94-96%, similar to gzip)
- âœ… **Fast decompression** (~200-400 MB/s, 2-4x faster than gzip)
- âœ… **Streaming support**
- âœ… **Adjustable compression levels** (1-22)

**Cons:**
- âŒ **Requires external library** (`pip install zstandard`)
- âŒ **Less universal** than gzip

**Performance Metrics:**
```
Compression:   ~200-300 MB/s (level 3)
Decompression: ~200-400 MB/s
Memory usage:  ~8-16 MB
Compression ratio: ~95% (400-500 MB from 8.2 GB)
```

**Best for:**
- Modern systems
- Balance of speed and compression
- When you control the environment

---

### 4. tar.gz (Archive) - Not Recommended for Direct Access

```bash
# Extract required
tar -xzf dsp20241104.tar.gz
```

**Pros:**
- âœ… **Standard archive format**
- âœ… **Good for distribution**

**Cons:**
- âŒ **Cannot stream** - must extract entire file
- âŒ **Extra overhead** (tar header + gzip)
- âŒ **Not suitable for sequential reading**

**Best for:**
- Distribution
- Backup storage
- Manual extraction

---

## Implementation: Add Compression Support

### Option 1: Smart File Opener (Recommended)

```python
# Add to twse_data_loader.py

import gzip
from pathlib import Path

class TWSEDataLoader:
    def __init__(self, file_path: str | Path):
        self.file_path = Path(file_path)
        self.parser = TWSESnapshotParser()
        
    def _open_file(self):
        """Smart file opener - auto-detects compression."""
        if self.file_path.suffix == '.gz':
            return gzip.open(self.file_path, 'rb')
        elif self.file_path.suffix == '.lz4':
            import lz4.frame
            return lz4.frame.open(self.file_path, 'rb')
        elif self.file_path.suffix == '.zst':
            import zstandard as zstd
            dctx = zstd.ZstdDecompressor()
            return dctx.stream_reader(open(self.file_path, 'rb'))
        else:
            return open(self.file_path, 'rb')
    
    def read_records(self, limit: Optional[int] = None):
        """Read records with automatic decompression."""
        count = 0
        with self._open_file() as f:
            while True:
                if limit is not None and count >= limit:
                    break
                
                record = f.read(191)  # Read one record
                if not record or len(record) < 190:
                    break
                
                snapshot = self.parser.parse_record(record)
                if snapshot:
                    yield snapshot
                    count += 1
```

**Usage:**
```python
# Works with any format!
loader = TWSEDataLoader('snapshot/dsp20241104.gz')     # gzip
loader = TWSEDataLoader('snapshot/dsp20241104.lz4')    # LZ4
loader = TWSEDataLoader('snapshot/dsp20241104')        # uncompressed
```

---

## Performance Analysis

### Memory Usage Comparison

| Format           | Decompression Buffer | Peak RAM  | Notes                      |
| ---------------- | -------------------- | --------- | -------------------------- |
| **Uncompressed** | 0                    | ~1-10 MB  | Only file buffers          |
| **gzip**         | 8-32 KB              | ~1-10 MB  | Streaming decompression    |
| **LZ4**          | 1-4 MB               | ~5-15 MB  | Larger buffer, still small |
| **Zstd**         | 8-16 MB              | ~10-20 MB | Slightly larger            |

**Key Insight**: All formats support **streaming** - they don't need to decompress the entire file into memory!

### Sequential Loading Performance

Testing with 8.2 GB file (~43M records):

| Format             | Read Speed   | Decompression CPU | Total Time | RAM Usage |
| ------------------ | ------------ | ----------------- | ---------- | --------- |
| **Uncompressed**   | 200-500 MB/s | 0%                | ~20s       | 10 MB     |
| **gzip**           | 50-100 MB/s  | ~50-80%           | ~100s      | 10 MB     |
| **LZ4**            | 500-800 MB/s | ~10-20%           | ~25s       | 15 MB     |
| **Zstd (level 3)** | 200-400 MB/s | ~20-40%           | ~30s       | 20 MB     |

**Observations:**
1. âœ… **RAM usage is minimal** for all formats (~10-20 MB)
2. âœ… **Sequential loading works** - no need to decompress entire file
3. âš ï¸ **gzip is slowest** but has best compression
4. âœ… **LZ4 is almost as fast** as uncompressed

### Conversion Performance

Creating compressed files from uncompressed:

```python
import gzip
import shutil

# Compress with gzip (level 6 - default)
with open('dsp20241104', 'rb') as f_in:
    with gzip.open('dsp20241104.gz', 'wb', compresslevel=6) as f_out:
        shutil.copyfileobj(f_in, f_out, length=16*1024*1024)  # 16MB chunks
```

| Format      | Compression Speed | Time for 8.2 GB | File Size       |
| ----------- | ----------------- | --------------- | --------------- |
| **gzip -6** | 100-150 MB/s      | ~60s            | 397 MB          |
| **gzip -9** | 50-80 MB/s        | ~120s           | 380 MB (better) |
| **LZ4**     | 300-500 MB/s      | ~20s            | 1.6-2 GB        |
| **Zstd -3** | 200-300 MB/s      | ~30s            | 400-500 MB      |

---

## Impact Analysis

### âœ… Benefits

1. **Storage Savings**
   - Reduce 8.2 GB â†’ 400 MB (gzip)
   - Save ~95% disk space
   - Daily files: 8.2 GB â†’ 400 MB
   - Monthly: 246 GB â†’ 12 GB
   - Yearly: ~3 TB â†’ ~150 GB

2. **Transfer Speed**
   - Network transfer: 20x less data
   - Backup time: 20x faster
   - S3/cloud storage costs: 20x cheaper

3. **No Memory Impact**
   - Streaming decompression
   - RAM usage: 10-20 MB (same as uncompressed)
   - Sequential loading preserved

### âš ï¸ Trade-offs

1. **CPU Overhead**
   - gzip: +50-80% CPU (moderate)
   - LZ4: +10-20% CPU (minimal)
   - Modern CPUs handle this easily

2. **Processing Time**
   - gzip: ~100s vs 20s (5x slower)
   - LZ4: ~25s vs 20s (1.25x slower)
   - **Still acceptable** for daily processing

3. **No Random Access**
   - Must read sequentially from start
   - Cannot jump to middle of file
   - **Not an issue** for time-series data

---

## Recommendations by Use Case

### 1. Production Backtesting â­
**Use: Parquet/Feather (pre-processed)**

```bash
# Pre-process once
python convert_to_feather_by_date.py

# Read instantly
df = pd.read_feather('feather_data/2024-11-11/0050.TWSE.feather')
```

**Reason:**
- Already compressed (~90% savings)
- Instant column access
- No decompression overhead during backtest

### 2. Raw Data Storage â­
**Use: gzip (.gz)**

```python
# Store compressed
loader = TWSEDataLoader('snapshot/dsp20241104.gz')
```

**Reason:**
- Best compression (95%)
- Native Python support
- Streaming works perfectly

### 3. Frequent Re-reading
**Use: LZ4 (.lz4)**

```python
# Fast repeated access
loader = TWSEDataLoader('snapshot/dsp20241104.lz4')
```

**Reason:**
- Fast decompression (5-10x faster than gzip)
- Still good compression (75%)
- Low CPU overhead

### 4. Development
**Use: Uncompressed**

**Reason:**
- No overhead
- Fast debugging
- Simple

---

## Practical Workflow

### Workflow A: Archive + On-demand Conversion

```bash
# 1. Store compressed archives
raw/
  â”œâ”€â”€ dsp20241104.gz  (397 MB)
  â”œâ”€â”€ dsp20241105.gz  (401 MB)
  â””â”€â”€ dsp20241106.gz  (395 MB)

# 2. Convert to feather when needed
python convert_to_feather_by_date.py snapshot/dsp20241104.gz

# 3. Fast analysis
df = pd.read_feather('feather_data/2024-11-04/0050.TWSE.feather')
```

**Benefits:**
- Minimal storage (400 MB/day compressed)
- Fast access (feather after conversion)
- Best of both worlds

### Workflow B: Hybrid Storage

```bash
# Recent data (last 7 days): Uncompressed/Feather for speed
recent/
  â””â”€â”€ feather_data/  (fast access)

# Historical data (older): Compressed for space
archive/
  â”œâ”€â”€ 2024-01/  (all .gz files)
  â”œâ”€â”€ 2024-02/
  â””â”€â”€ ...
```

### Workflow C: Direct Compressed Reading

```python
# Just read compressed files directly
loader = TWSEDataLoader('snapshot/dsp20241104.gz')
snapshots = list(loader.read_records())

# Convert to DataFrame
df = pd.DataFrame([s.to_dict() for s in snapshots])
```

**When to use:**
- One-time analysis
- Exploration
- Don't want to pre-process

---

## Compression Utilities

### Create Compressed Files

```python
import gzip
import shutil

def compress_file(input_path, output_path, compresslevel=6):
    """Compress binary file with gzip."""
    with open(input_path, 'rb') as f_in:
        with gzip.open(output_path, 'wb', compresslevel=compresslevel) as f_out:
            shutil.copyfileobj(f_in, f_out, length=16*1024*1024)

# Usage
compress_file('snapshot/dsp20241104', 'snapshot/dsp20241104.gz')
```

### Batch Compression

```bash
# Compress all files in directory
for file in snapshot/dsp*; do
    if [[ ! -f "$file.gz" ]]; then
        echo "Compressing $file..."
        gzip -6 -k "$file"  # -k keeps original
    fi
done
```

---

## Summary Table

| Metric              | Uncompressed | gzip    | LZ4      | Zstd     | Parquet/Feather |
| ------------------- | ------------ | ------- | -------- | -------- | --------------- |
| **File Size**       | 8.2 GB       | 397 MB  | 1.6 GB   | 450 MB   | 500 MB          |
| **Read Speed**      | 500 MB/s     | 80 MB/s | 700 MB/s | 350 MB/s | Instant         |
| **RAM Usage**       | 10 MB        | 10 MB   | 15 MB    | 20 MB    | 50-100 MB       |
| **CPU Usage**       | Low          | High    | Low      | Medium   | Low             |
| **Sequential**      | âœ…            | âœ…       | âœ…        | âœ…        | âœ…               |
| **Random Access**   | âŒ            | âŒ       | âŒ        | âŒ        | âœ…               |
| **Python Built-in** | âœ…            | âœ…       | âŒ        | âŒ        | âŒ (pandas)      |
| **Best For**        | Debug        | Archive | Speed    | Balance  | Queries         |

---

## Conclusion

### Key Findings

1. âœ… **Compression works great** for TWSE data
   - 95% space savings (gzip)
   - Streaming decompression (no RAM explosion)
   - Sequential loading preserved

2. âœ… **No significant downsides**
   - RAM usage: Same (~10-20 MB)
   - CPU overhead: Acceptable (50-80% for gzip)
   - Processing time: 5x slower (100s vs 20s) but still reasonable

3. âœ… **Recommended approach**
   - **Storage**: gzip (.gz) for archives
   - **Processing**: Convert to Feather/Parquet
   - **Development**: Use compressed files directly

### Final Recommendation

**Use gzip compression** for raw TWSE binary files:

```python
# Enhanced loader with compression support
loader = TWSEDataLoader('snapshot/dsp20241104.gz')
snapshots = list(loader.read_records())
```

**Benefits:**
- ðŸ’¾ Save 95% storage (8.2 GB â†’ 397 MB)
- ðŸš€ No RAM impact (streaming decompression)
- âš¡ Acceptable speed (80-100 MB/s)
- ðŸ“¦ Native Python support (no external deps)
- âœ… Works with existing code

**Implementation:** See updated `twse_data_loader.py` with compression support.

---

## References

- [Python gzip module](https://docs.python.org/3/library/gzip.html)
- [LZ4 Frame Format](https://github.com/lz4/lz4)
- [Zstandard](https://github.com/facebook/zstd)
- [Compression Benchmark](https://github.com/inikep/lzbench)

